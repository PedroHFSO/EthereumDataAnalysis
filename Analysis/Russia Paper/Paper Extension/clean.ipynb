{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The .csv file was obtained from [EthereumETL](https://github.com/blockchain-etl/ethereum-etl), but it has a lot of columns we are not interested in. Then, we are going to remove them from our final .csv files in order to reduce memory and storage usage.\n",
    "\n",
    "The columns kept are the following:\n",
    "\n",
    "| **Column**      | **Description**                                               |\n",
    "| --------------- | ------------------------------------------------------------- |\n",
    "| hash            | Identifier hash of the transaction                            |\n",
    "| block_number    | Number of the block that contains that transaction in decimal |\n",
    "| block_timestamp | UNIX Timestamp in UTC that shows when the block was mined     |\n",
    "| from_address    | Identifier hash of the account that sent the transaction      |\n",
    "| to_address      | Identifier hash of the account that received the transaction  |\n",
    "| value           | Amount of ETH sent in the transaction                         |\n",
    "\n",
    "These columns are sufficient to check things like:\n",
    "\n",
    "* Transaction volume in function of time;\n",
    "* Graph metrics, building a graph of accounts as nodes and transactions as edges;\n",
    "* Possibility to weigh the edges of the graph using the _value_ column.\n",
    "\n",
    "---\n",
    "\n",
    "### Filenames\n",
    "\n",
    "The raw files were named as \"MMyy_id.csv\", where MM is the month name in lowercase, yy is the 2 last digits of the year and id is a unique identifier for the file. The lightweight files were named with the same format, but with \"light-\" at the start."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_df(month_name, year_name, dataset_id, relative_path):\n",
    "    DATASET_NAME = month_name+year_name+'_'+dataset_id+'.csv'\n",
    "    df = dd.read_csv(DATASET_NAME)\n",
    "    df = df[['hash', 'block_number','block_timestamp','from_address','to_address','value']]\n",
    "    df.to_csv('light-'+DATASET_NAME, single_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_light_df('july', '21', '1', './')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
